{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def7caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e59ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "data_train='/Users/JuanVillalba/Desktop/ImageProcess/CarneDataset/train'\n",
    "data_test='/Users/JuanVillalba/Desktop/ImageProcess/CarneDataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3002615",
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas=20 #numero de veces para el entrenamiento\n",
    "altura, longitud=100, 100 #tamano de pixeles\n",
    "batch_size=32 #número de imágenes que vamos a procesar durante el entrenamiento\n",
    "pasos=1000 #número de veces que se va a procesar la información durante las épocas\n",
    "pasos_validacion=200 # cuantas veces se van a ejecutar los pasos de validación\n",
    "filtrosConv1=32 #profundidad de 32 \n",
    "filtrosConv2=64 #profundidad de 64\n",
    "tamano_filtro1=[3,3]\n",
    "tamano_filtro2=[2,2]\n",
    "tamano_pool=[2,2]\n",
    "clases=8 #cantidad de carpetas para la clasificación\n",
    "lr=0.005 #Ajustes de la red neuronal para acercar a una óptima revisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611af335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesamiento de imágenes\n",
    "\n",
    "train_datagen=ImageDataGenerator(\n",
    "rescale=1./255, #Valores pixeles de la imagen para mejorar el entrenamiento\n",
    "shear_range=0.3, #Generar imágenes inclinada\n",
    "zoom_range=0.3, #Algunas hará zoom\n",
    "horizontal_flip=True #Para distinguir direccionalidad de la imagen\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec56cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(\n",
    "\trescale=1./255 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28c691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1633 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "#Procesa todas las imágenes de todas las carpetas\n",
    "image_train = train_datagen.flow_from_directory(\n",
    "\tdata_train,\n",
    "\ttarget_size=(altura, longitud),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e52570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 810 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "image_test=test_datagen.flow_from_directory(\n",
    "\tdata_test,\n",
    "\ttarget_size=(altura, longitud),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66afdc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 11:22:42.922563: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Crear la red CNN\n",
    "cnn=Sequential()\n",
    "cnn.add(Convolution2D(filtrosConv1, \n",
    "\ttamano_filtro1, \n",
    "\tpadding='same', \n",
    "\tinput_shape=(altura,longitud,3), \n",
    "\tactivation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "cnn.add(Convolution2D(filtrosConv2, \n",
    "\ttamano_filtro1, padding='same', \n",
    "\tinput_shape=(altura,longitud,3), \n",
    "\tactivation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47dab631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 100, 100, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 50, 50, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 25, 25, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,392\n",
      "Trainable params: 19,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2baea6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplanar información\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(256,activation='relu')) #Añade una capa más de 256 neuronas en activación relu que es la unidad lineal rectificada.\n",
    "cnn.add(Dropout(0.5))#Durante el entrenamiento se apagará el 50% de las neuronas en cada paso de las 256, caminos alternos para revisar la data.\n",
    "cnn.add(Dense(clases, activation='softmax')) #El valor de porcentaje más alto de identificación de clase clasificará la imagen\n",
    "cnn.compile(loss='categorical_crossentropy', \n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa3113cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 100, 100, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 50, 50, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 25, 25, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 40000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               10240256  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,261,704\n",
      "Trainable params: 10,261,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4208df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/75/z2s0x66143n_008wgn0h_z9h0000gp/T/ipykernel_7729/188802715.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history=cnn.fit_generator(image_train,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  52/1000 [>.............................] - ETA: 4:06 - loss: 2.6521 - accuracy: 0.5487WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20000 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "1000/1000 [==============================] - 19s 18ms/step - loss: 2.6521 - accuracy: 0.5487 - val_loss: 1.4409 - val_accuracy: 0.5667\n"
     ]
    }
   ],
   "source": [
    "history=cnn.fit_generator(image_train, \n",
    "        steps_per_epoch=pasos, \n",
    "        epochs=epocas, \n",
    "        validation_data=image_test, \n",
    "        validation_steps=pasos_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea19388",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/Users/JuanVillalba/Desktop/ImageProcess/modelo/'\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "\tos.mkdir(dir)\n",
    "cnn.save('/Users/JuanVillalba/Desktop/ImageProcess/modelo.h5')\n",
    "cnn.save_weights('/Users/JuanVillalba/Desktop/ImageProcess//modelo/pesos.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0af98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
